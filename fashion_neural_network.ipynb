{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import du dataset présent dans Keras\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(images, targets), (_, _) = fashion_mnist.load_data()\n",
    "\n",
    "#Get subpart of the dataset\n",
    "images = images[:10000]\n",
    "targets = targets[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking on of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "                 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
    "\n",
    "#Plot one image\n",
    "plt.imshow(images[1], cmap = 'binary')\n",
    "plt.title(targets_names[targets[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#Applatissement de l'image (Flatten): images en 28*28 = 784\n",
    "#model.add(tf.keras.layers.Flatten(input_shape = [28,28]))\n",
    "images = images.reshape(-1, 784)\n",
    "images = images.astype(float)\n",
    "\n",
    "#Normalisation (images = (images - np.mean(images)) / np.std(images))\n",
    "images = StandardScaler().fit_transform(images)\n",
    "\n",
    "print(images.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 784) (8000,)\n",
      "(2000, 784) (2000,)\n"
     ]
    }
   ],
   "source": [
    "#Spliting the dataset into train_set and test_set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "images_train, images_test, targets_train, targets_test = train_test_split(images, targets, test_size = 0.2, random_state =  42)\n",
    "\n",
    "print(images_train.shape, targets_train.shape)\n",
    "print(images_test.shape, targets_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.25079268 0.06381431 0.16733965 0.03051468 0.15194176 0.0371086\n",
      "  0.1415693  0.05582154 0.07654673 0.02455075]], shape=(1, 10), dtype=float32) [9]\n"
     ]
    }
   ],
   "source": [
    "#Création d'un model séquentiel\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Ajouts des layers\n",
    "model.add(tf.keras.layers.Dense(256, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "model_output = model(images[0:1])\n",
    "print(model_output, targets[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (1, 256)                  200960    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, 128)                  32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (1, 64)                   8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (1, 10)                   650       \n",
      "=================================================================\n",
      "Total params: 242,762\n",
      "Trainable params: 242,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "             optimizer = 'sgd',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model on the train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 18s 3ms/step - loss: 0.7032 - accuracy: 0.7511 - val_loss: 0.5277 - val_accuracy: 0.8037\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 18s 3ms/step - loss: 0.5016 - accuracy: 0.8219 - val_loss: 0.5749 - val_accuracy: 0.7912\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 14s 2ms/step - loss: 0.4209 - accuracy: 0.8477 - val_loss: 0.5063 - val_accuracy: 0.8194\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 14s 2ms/step - loss: 0.3999 - accuracy: 0.8572 - val_loss: 0.5469 - val_accuracy: 0.8219\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.3670 - accuracy: 0.8680 - val_loss: 0.4983 - val_accuracy: 0.8325\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.3178 - accuracy: 0.8802 - val_loss: 0.5559 - val_accuracy: 0.8363\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.2707 - accuracy: 0.8988 - val_loss: 0.5465 - val_accuracy: 0.8313\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.2725 - accuracy: 0.9019 - val_loss: 0.5314 - val_accuracy: 0.8331\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.2355 - accuracy: 0.9147 - val_loss: 0.5283 - val_accuracy: 0.8406\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.2272 - accuracy: 0.9153 - val_loss: 0.5225 - val_accuracy: 0.8369\n"
     ]
    }
   ],
   "source": [
    "history = model.fit (images_train, targets_train, epochs = 10,\n",
    "                     batch_size=1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5024 - accuracy: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5023539662361145, 0.8500000238418579]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(images_test, targets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predict that the image is a Ankle Boot, when in reality, it is a Ankle Boot\n"
     ]
    }
   ],
   "source": [
    "targets_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "                 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
    "\n",
    "model_output = model.predict(images[0:1])\n",
    "\n",
    "for i in model_output:\n",
    "    for j in i:\n",
    "        if j<0.5:\n",
    "            pass\n",
    "        else:\n",
    "            model_output = list(i).index(j)\n",
    "\n",
    "print(f'Model predict that the image is a {targets_names[model_output]}, when in reality, it is a {targets_names[targets[0:1][0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\m2lan\\anaconda3\\lib\\site-packages (3.1.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.17.5; python_version == \"3.8\" in c:\\users\\m2lan\\anaconda3\\lib\\site-packages (from h5py) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "#Save the model\n",
    "#model.save('fashion_nn')\n",
    "\n",
    "#Lad the model\n",
    "#loaded_model = tf.keras.models.load_model('fashion_nn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
